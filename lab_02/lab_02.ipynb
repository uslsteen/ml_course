{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Первое задание.\n",
        "## Работу выполнил: Шурыгин Антон, Б01 - 909."
      ],
      "metadata": {
        "id": "mcUAerKRf51U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JEbyPOy8WqEd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9c226522-7401-4d68-8f0c-4ac210589b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.9.24)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.0\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подтягиваем датасет с kaggle"
      ],
      "metadata": {
        "id": "woO9VddXlq-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "\n",
        "api_token = {\"username\":\"uslsteen\", \"key\":\"56dbf05759db832787aa95054b6b5a45\"}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "UZrAyh5JD9nh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c fall-ml2-mipt-2022 --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "49VvRE0gNlC6",
        "outputId": "1030307a-e9e9-4db8-d47f-611123f1953e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading fall-ml2-mipt-2022.zip to /content\n",
            " 76% 30.0M/39.3M [00:00<00:00, 160MB/s] \n",
            "100% 39.3M/39.3M [00:00<00:00, 138MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/fall-ml2-mipt-2022.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XWECcLL-OUm2",
        "outputId": "ccfb35b4-f8e9-49fe-dd6d-d8213ad4ece9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/fall-ml2-mipt-2022.zip\n",
            "  inflating: sample.csv              \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подключаем небоходимые модули"
      ],
      "metadata": {
        "id": "NiPoBZI3gBvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "import torchmetrics\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics.functional as FM\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "s3zkLvgWCpTA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Определим класс **DataWrapper** - класс владеющий и управляющий загруженным датасетом."
      ],
      "metadata": {
        "id": "1yPBvav5gHa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataWrapper():\n",
        "  def __init__(self, train_path : str = str(), test_path : str = str(),  feature_vars : list = list(), target_vars : list = list()):\n",
        "  #\n",
        "    self.feature_vars = feature_vars\n",
        "    self.target_vars = target_vars\n",
        "    self.all_vars = feature_vars + target_vars\n",
        "    #\n",
        "    train_ds = pd.read_csv(train_path)\n",
        "    test_ds = pd.read_csv(test_path)\n",
        "    #\n",
        "    (self.feature_train, self.target_train) = slice_data(train_ds, feature_vars, target_vars)\n",
        "    self.feature_test = slice_data(test_ds, feature_vars) \n",
        "    # \n",
        "    self.dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.scaler = StandardScaler()\n",
        "    #\n",
        "  #\n",
        "  def cast(self, data, is_target : bool = True):\n",
        "  #  \n",
        "    casted_data = None\n",
        "    #\n",
        "    if is_target == True:\n",
        "      data = data.reshape(-1, 1, 10, 10) * 10\n",
        "      data = np.repeat(data, 3, axis=1)\n",
        "      casted_data = torch.tensor(data).float().to(self.dev)\n",
        "    else:\n",
        "      casted_data = torch.tensor(data).type(torch.LongTensor).to(self.dev)\n",
        "    #\n",
        "    return casted_data\n",
        "    #\n",
        "  #\n",
        "  def split_data(self):\n",
        "    X_train, X_test, Y_train, Y_test = split(self.feature_train, self.target_train, 0.1, 3)\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "    #\n",
        "  #\n",
        "  def make_preproc(self, is_train : bool = True):\n",
        "  #\n",
        "    if is_train == True:\n",
        "      X_train, X_test, Y_train, Y_test = self.split_data()\n",
        "      #\n",
        "      self.scaler.fit(X_train)\n",
        "      #\n",
        "      X_train = self.scaler.transform(X_train)\n",
        "      X_train = self.cast(X_train)\n",
        "      Y_train = self.cast(Y_train, is_target = False)\n",
        "      #\n",
        "      X_test = self.scaler.transform(X_test)\n",
        "      X_test = self.cast(X_test)\n",
        "      Y_test = self.cast(Y_test, is_target = False)\n",
        "      return X_train, Y_train, X_test, Y_test\n",
        "    # NOTE: if not train preprocessing\n",
        "    else: \n",
        "      X_test = self.scaler.transform(self.feature_test)\n",
        "      X_test = self.cast(X_test)\n",
        "      return X_test \n",
        "    #\n",
        "  # NOTE: custom feature data scaller\n",
        "  def scale_features(self):\n",
        "    sc_feature = StandardScaler()\n",
        "    #\n",
        "    self.features_test = sc_feature.fit_transform(self.features_test)\n",
        "    self.features_train = sc_feature.transform(self.features_train)\n",
        "    #\n",
        "  #\n",
        "  def scale_target(self):\n",
        "    sc_target = StandardScaler()\n",
        "    #\n",
        "    self.target_test = sc_target.fit_transform(self.target_test)\n",
        "    self.target_train = sc_target.transform(self.target_train)\n",
        "    #\n",
        "  #\n",
        "def slice_data(data, features_slice : list = list(), target_slice : list = list()):\n",
        "  if target_slice != list():\n",
        "    return data.loc[:, features_slice[0]:features_slice[1]].to_numpy(), data.loc[:, target_slice[0]].to_numpy()\n",
        "  else:\n",
        "    return data.loc[:, features_slice[0]:features_slice[1]].to_numpy()\n",
        "  #\n",
        "#\n",
        "# NOTE: custom dataset splitter\n",
        "def split(X, Y, testsize : float, random_st : int):\n",
        "#\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = testsize, random_state=random_st)\n",
        "  return X_train, X_test, Y_train, Y_test\n",
        "  #\n",
        "#"
      ],
      "metadata": {
        "id": "9gG9Y9ncPyMv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Определим СustomDataset -- прокси - класс для хранения для датасета.\n",
        "- Наследуемся от ```torch.utils.data.Dataset``` для последующей передачи в конструктор ```DataLoader```"
      ],
      "metadata": {
        "id": "ICbyQPV0g9SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class СustomDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, feature, target):\n",
        "    self.feature = feature\n",
        "    self.target = target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target)\n",
        "  \n",
        "  def __getitem__(self, ind):\n",
        "    return self.feature[ind], self.target[ind]"
      ],
      "metadata": {
        "id": "DFoprKDYVVHH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cоздаем wrapper, загружаем нужные .csv файлы и достаем признаки ```[x_0, x_99]```, a так же таргет - переменную - ```Category```\n",
        "- Количество объектов признаков в размере 100 - результат перебора разных размеров датасета. "
      ],
      "metadata": {
        "id": "H_Th8Lz3hyJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/'\n",
        "wrapper = DataWrapper(path + 'train.csv', path + 'test.csv', ['x_0', 'x_99'], ['Category'])"
      ],
      "metadata": {
        "id": "UVtFWh3WAl-T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Далее - препроцессинг данных.\n",
        "- Дефолтный булевый флаг высталвен в True => работаем с train.csv датасаетом.\n",
        "- Сплитом разбиваем данные и кастуем все нужно к ```torch.tensor``` (а именно feature_train, target_train и feature_test, target_test - numpy матрицы, взятые непосредственно из train.csv датасета).\n",
        "- Дамп размеров полученных тензоров для самопроверки."
      ],
      "metadata": {
        "id": "CjQga7J5iL3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_train, feature_train, target_test, feature_test = wrapper.make_preproc()\n",
        "#\n",
        "print(f\"size = {target_train.size()}\")\n",
        "print(f'size = {feature_train.size()}')\n",
        "print(f\"size = {target_test.size()}\")\n",
        "print(f'size = {feature_test.size()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WLmzodjTTUCq",
        "outputId": "82d3dbd0-a7ab-4c8b-b7ca-aeea2efe98ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size = torch.Size([9000, 3, 10, 10])\n",
            "size = torch.Size([9000])\n",
            "size = torch.Size([1000, 3, 10, 10])\n",
            "size = torch.Size([1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Разбитые на train и test данные отправляем в конструктор кастомного датасета\n",
        "- А затем отправляем в конструктор DataLoader, чтобы получить нативный доступ к преобразованным семплам."
      ],
      "metadata": {
        "id": "WQZPxWFEjMfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = СustomDataset(target_train, feature_train)\n",
        "test_ds = СustomDataset(target_test, feature_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_ds, batch_size=1000, shuffle=False)"
      ],
      "metadata": {
        "id": "ihni2lIcVewu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Определяем кастомный классификатора.\n",
        "- Изначально выбор resnet34 в качестве модели объясняется огромным количеством признаков. \n",
        "- После шаманства с количеством признаков объектов и их уменьшением в конечном итоге было принято решение оставить как есть. Пусть, это, вероятно и оверхед - использование нейронки для изображений в данном случае."
      ],
      "metadata": {
        "id": "StttdOl2juMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "#\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = models.resnet34(num_classes=32)\n",
        "    self.accuracy = torchmetrics.Accuracy('multiclass', num_classes=32)\n",
        "    self.learning_rate = 1e-3\n",
        "    #\n",
        "  #\n",
        "  def forward(self, X):\n",
        "    return self.model(X)\n",
        "    #\n",
        "  #\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.parameters(), self.learning_rate)"
      ],
      "metadata": {
        "id": "dl9149mkVjTY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(classifier, train_dataloader, test_dataloader, train_ds : СustomDataset, test_ds : СustomDataset,\n",
        "                    optimizer, max_epochs=10, device = None):\n",
        "  classifier.to(device)\n",
        "\n",
        "  opt_model = classifier\n",
        "  opt_acc = 0\n",
        "\n",
        "  for epoch in range(0, max_epochs):\n",
        "    classifier.train()\n",
        "    for i, (features, label) in enumerate(train_dataloader):\n",
        "      features, label = features.to(device), label.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      pred = classifier(features)\n",
        "      loss = F.cross_entropy(pred, label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    train_accur = classifier.accuracy(torch.argmax(classifier(train_ds.feature), 1), \n",
        "                                         torch.tensor(train_ds.target).to(device))\n",
        "    #\n",
        "    test_accur = classifier.accuracy(torch.argmax(classifier(test_ds.feature), 1), \n",
        "                                         torch.tensor(test_ds.target).to(device))\n",
        "    \n",
        "    if (test_accur > opt_acc):\n",
        "      opt_model = classifier\n",
        "      opt_acc = test_accur\n",
        "\n",
        "    print(f\"[{epoch + 1}/{max_epochs}]: train accur. = {train_accur}, test accur. = {test_accur}\")\n",
        "  \n",
        "  return opt_model"
      ],
      "metadata": {
        "id": "PeXYK3h-Vj9a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = Classifier()\n",
        "optimizer = classifier.configure_optimizers()\n",
        "\n",
        "model = fit(classifier, train_dataloader, test_dataloader, train_ds, test_ds, optimizer, max_epochs = 50, device = wrapper.dev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BzlYKlH7VnA1",
        "outputId": "93da8c75-9770-44b6-9812-a61cde1409af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-156a41c870cc>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(train_ds.target).to(device))\n",
            "<ipython-input-12-156a41c870cc>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(test_ds.target).to(device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/50]: train accur. = 0.18244443833827972, test accur. = 0.16300000250339508\n",
            "[2/50]: train accur. = 0.2508888840675354, test accur. = 0.23899999260902405\n",
            "[3/50]: train accur. = 0.32422223687171936, test accur. = 0.2720000147819519\n",
            "[4/50]: train accur. = 0.363444447517395, test accur. = 0.3140000104904175\n",
            "[5/50]: train accur. = 0.36133334040641785, test accur. = 0.3059999942779541\n",
            "[6/50]: train accur. = 0.4020000100135803, test accur. = 0.33399999141693115\n",
            "[7/50]: train accur. = 0.42311111092567444, test accur. = 0.3659999966621399\n",
            "[8/50]: train accur. = 0.4234444499015808, test accur. = 0.35899999737739563\n",
            "[9/50]: train accur. = 0.4372222125530243, test accur. = 0.36399999260902405\n",
            "[10/50]: train accur. = 0.4487777650356293, test accur. = 0.367000013589859\n",
            "[11/50]: train accur. = 0.4663333296775818, test accur. = 0.38499999046325684\n",
            "[12/50]: train accur. = 0.460999995470047, test accur. = 0.3880000114440918\n",
            "[13/50]: train accur. = 0.4714444577693939, test accur. = 0.4020000100135803\n",
            "[14/50]: train accur. = 0.47822222113609314, test accur. = 0.40799999237060547\n",
            "[15/50]: train accur. = 0.47866666316986084, test accur. = 0.3869999945163727\n",
            "[16/50]: train accur. = 0.5080000162124634, test accur. = 0.40700000524520874\n",
            "[17/50]: train accur. = 0.5045555830001831, test accur. = 0.4180000126361847\n",
            "[18/50]: train accur. = 0.4985555410385132, test accur. = 0.3959999978542328\n",
            "[19/50]: train accur. = 0.5121111273765564, test accur. = 0.4169999957084656\n",
            "[20/50]: train accur. = 0.5202222466468811, test accur. = 0.40799999237060547\n",
            "[21/50]: train accur. = 0.526888906955719, test accur. = 0.41499999165534973\n",
            "[22/50]: train accur. = 0.5372222065925598, test accur. = 0.4099999964237213\n",
            "[23/50]: train accur. = 0.5424444675445557, test accur. = 0.4099999964237213\n",
            "[24/50]: train accur. = 0.539222240447998, test accur. = 0.4099999964237213\n",
            "[25/50]: train accur. = 0.5312222242355347, test accur. = 0.421999990940094\n",
            "[26/50]: train accur. = 0.5505555272102356, test accur. = 0.40700000524520874\n",
            "[27/50]: train accur. = 0.5632222294807434, test accur. = 0.4269999861717224\n",
            "[28/50]: train accur. = 0.5723333358764648, test accur. = 0.41200000047683716\n",
            "[29/50]: train accur. = 0.5773333311080933, test accur. = 0.41200000047683716\n",
            "[30/50]: train accur. = 0.5804444551467896, test accur. = 0.41200000047683716\n",
            "[31/50]: train accur. = 0.5954444408416748, test accur. = 0.4269999861717224\n",
            "[32/50]: train accur. = 0.605222225189209, test accur. = 0.41600000858306885\n",
            "[33/50]: train accur. = 0.6150000095367432, test accur. = 0.4189999997615814\n",
            "[34/50]: train accur. = 0.632777750492096, test accur. = 0.4180000126361847\n",
            "[35/50]: train accur. = 0.6445555686950684, test accur. = 0.41999998688697815\n",
            "[36/50]: train accur. = 0.6517778038978577, test accur. = 0.41600000858306885\n",
            "[37/50]: train accur. = 0.628333330154419, test accur. = 0.4059999883174896\n",
            "[38/50]: train accur. = 0.6512222290039062, test accur. = 0.40700000524520874\n",
            "[39/50]: train accur. = 0.6647777557373047, test accur. = 0.4189999997615814\n",
            "[40/50]: train accur. = 0.6862221956253052, test accur. = 0.4189999997615814\n",
            "[41/50]: train accur. = 0.695111095905304, test accur. = 0.41999998688697815\n",
            "[42/50]: train accur. = 0.6846666932106018, test accur. = 0.40799999237060547\n",
            "[43/50]: train accur. = 0.7044444680213928, test accur. = 0.414000004529953\n",
            "[44/50]: train accur. = 0.7279999852180481, test accur. = 0.41200000047683716\n",
            "[45/50]: train accur. = 0.7293333411216736, test accur. = 0.4180000126361847\n",
            "[46/50]: train accur. = 0.7452222108840942, test accur. = 0.4169999957084656\n",
            "[47/50]: train accur. = 0.7049999833106995, test accur. = 0.4099999964237213\n",
            "[48/50]: train accur. = 0.7453333139419556, test accur. = 0.3930000066757202\n",
            "[49/50]: train accur. = 0.7557777762413025, test accur. = 0.414000004529953\n",
            "[50/50]: train accur. = 0.7635555267333984, test accur. = 0.40799999237060547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, _, Y_test = wrapper.split_data()\n",
        "#\n",
        "model.accuracy(torch.argmax(model(target_test), 1), torch.tensor(Y_test).to(wrapper.dev))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EF2nXLoza62w",
        "outputId": "a973e9cc-cab8-46fd-eaae-d1d2f0d49140"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4080, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Дампим результаты"
      ],
      "metadata": {
        "id": "wTN-ef8tpkCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dump(model):\n",
        "  feature_test = wrapper.make_preproc(is_train = False)\n",
        "  preds = torch.argmax(model(feature_test), 1).cpu().numpy()\n",
        "\n",
        "  with open('answer.csv', 'w') as output:\n",
        "    output.write('Id,Category')\n",
        "    for id, prediction in enumerate(preds):\n",
        "      output.write(f'\\n{id},{int(prediction)}')"
      ],
      "metadata": {
        "id": "OkyllL16U1yd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump(model)"
      ],
      "metadata": {
        "id": "0VghKnmjU37t"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}